from qa_assistant import SimpleChatBot, ContextualChatBot
from vector_builder import MedicalEmbeddings
from time import time
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from tqdm import tqdm
import os
from scipy import stats
from fuzzywuzzy import fuzz
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

current_file_dir = os.path.dirname(__file__)
project_root_dir = os.path.dirname(current_file_dir)

def calculate_response_time(chatbot, num_trials=10):
    """Measure the average time taken by the chatbot to generate responses.

    Args:
        chatbot (ChatBot): The chatbot instance to evaluate.
        num_trials (int, optional): Number of queries to test. Defaults to 10.
    
    Returns:
        tuple: Mean and standard deviation of response times in seconds.
    """
    response_times = []
    sample_query = "What are the important relationships between HER-2/neu and breast cancer?"
    
    for _ in range(num_trials):
        start_time = time()
        try:
            chatbot_response = chatbot.respond(sample_query)
        except Exception as e:
            logging.error(f"Error during response generation: {e}")
            continue
        end_time = time()
        response_times.append(end_time - start_time)
    
    return np.mean(response_times), np.std(response_times)

def is_answer_in_response(expected_answer, chatbot_response, threshold=80):
    """Check if the expected answer is present in the chatbot's response using fuzzy match.

    Args:
        expected_answer (str): The correct answer to the query.
        chatbot_response (str): The response generated by the chatbot.
        threshold (int): Similarity score threshold for acceptance.

    Returns:
        bool: True if the similarity score is above the threshold.
    """
    similarity = fuzz.partial_ratio(expected_answer.lower(), chatbot_response.lower())
    if similarity < threshold:
        logging.debug(f"\nExpected: {expected_answer}\nResponse: {chatbot_response}\nScore: {similarity}")
    return similarity >= threshold


def assess_factual_accuracy(chatbot, qa_dataset):
    """Evaluate the factual accuracy of the chatbot's responses.

    Args:
        chatbot (ChatBot): The chatbot instance to evaluate.
        qa_dataset (DataFrame): A dataset containing questions and their correct answers.

    Returns:
        float: The proportion of questions answered correctly.
    """
    questions = qa_dataset['Query'].values
    correct_answers = qa_dataset['Answer'].values
    responses = []

    for question in questions:
        try:
            response = chatbot.respond(question)
            responses.append(response)
        except Exception as e:
            logging.error(f"Error during response generation for question '{question}': {e}")
            responses.append("")

    accuracy = [
        is_answer_in_response(correct_answers[i], responses[i])
        for i in range(len(questions))
    ]
    return np.mean(accuracy)

def bootstrap_accuracy_interval(chatbot, qa_dataset, num_bootstrap=25, confidence=0.95):
    """Compute a confidence interval for the chatbot's accuracy using bootstrapping.

    Args:
        chatbot (ChatBot): The chatbot instance to evaluate.
        qa_dataset (DataFrame): A dataset containing questions and their correct answers.
        num_bootstrap (int, optional): Number of bootstrap samples. Defaults to 25.
        confidence (float, optional): Confidence level. Defaults to 0.95.

    Returns:
        tuple: Mean accuracy, lower bound, and upper bound of the confidence interval.
    """
    num_questions = len(qa_dataset)
    accuracy_scores = []
    
    for _ in range(num_bootstrap):
        sampled_indices = np.random.choice(num_questions, size=num_questions, replace=True)
        sampled_dataset = qa_dataset.iloc[sampled_indices].copy()
        accuracy_scores.append(assess_factual_accuracy(chatbot, sampled_dataset))
    
    mean_accuracy = np.mean(accuracy_scores)
    alpha = 1 - confidence
    t_value = stats.t.ppf(1 - alpha / 2, df=num_bootstrap - 1)
    standard_error = np.std(accuracy_scores, ddof=1) / np.sqrt(num_bootstrap)
    return mean_accuracy, t_value * standard_error

def main():
    """Main function to evaluate chatbots."""
    chatbot_names = [
        'Simple llama3.2', 'Simple llama3.2:1b', 
        'Contextual llama3.2', 'Contextual llama3.2:1b', 
        'Clinical Contextual llama3.2', 'Clinical Contextual llama3.2:1b'
    ]
    chatbots = [
        SimpleChatBot(), SimpleChatBot('llama3.2:1b'), 
        ContextualChatBot(), ContextualChatBot('llama3.2:1b'), 
        ContextualChatBot(use_clinical_embeddings=True), 
        ContextualChatBot('llama3.2:1b', use_clinical_embeddings=True)
    ]
    
    factual_questions_path = os.path.join(project_root_dir, 'reference_docs', 'factual_questions.csv')
    if not os.path.exists(factual_questions_path):
        logging.error(f"Factual questions file not found at {factual_questions_path}")
        return

    factual_questions = pd.read_csv(factual_questions_path)

    avg_response_times = []
    response_time_stds = []
    factual_accuracy_scores = []
    accuracy_error_bars = []
    num_trials = 25

    for i, chatbot in tqdm(enumerate(chatbots)):
        mean_time, std_time = calculate_response_time(chatbot, num_trials)
        avg_response_times.append(mean_time)
        response_time_stds.append(std_time)
        logging.info(f"\n{chatbot_names[i]}")
        logging.info(f"Average Response Time: {mean_time} seconds; Std Dev: {std_time}")
        
        accuracy, interval = bootstrap_accuracy_interval(chatbot, factual_questions, num_bootstrap=10)
        factual_accuracy_scores.append(accuracy)
        accuracy_error_bars.append(interval)
        logging.info(f"Factual Accuracy: {accuracy} Â± {interval}")
    
    # Plot factual accuracy
    plt.bar(chatbot_names, factual_accuracy_scores)
    plt.errorbar(chatbot_names, factual_accuracy_scores, yerr=accuracy_error_bars, capsize=5, ecolor='black', elinewidth=1.5, capthick=1.5, fmt='none')
    plt.xlabel('Chatbot')
    plt.xticks(rotation=45)
    plt.ylabel('Proportion of Correct Answers')
    plt.savefig('factual_accuracy_plot.png', dpi=180, bbox_inches="tight")

    
    # Plot response times
    plt.figure()
    response_time_intervals = 1.96 * np.array(response_time_stds) / np.sqrt(num_trials)
    plt.bar(chatbot_names, avg_response_times)
    plt.errorbar(chatbot_names, avg_response_times, yerr=response_time_intervals, capsize=5, ecolor='black', elinewidth=1.5, capthick=1.5, fmt='none')
    plt.xlabel('Chatbot')
    plt.xticks(rotation=45)
    plt.ylabel('Average Response Time (seconds)')
    plt.savefig('response_time_plot.png', dpi=180, bbox_inches="tight")
    print("Saved response_time_plot.png")

if __name__ == '__main__':
    main()
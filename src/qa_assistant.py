import argparse
import ollama
from abc import ABC, abstractmethod
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import pickle
import warnings
from vector_builder import MedicalEmbeddings
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

current_dir = os.path.dirname(__file__)
root_dir = os.path.dirname(current_dir)

DEFAULT_PROMPT = "You are an expert chatbot designed to answer questions about a " \
    "scientific paper on human breast cancer and the HER-2 oncogene. "\
    "Respond only to relevant queries and politely redirect users asking "\
    "irrelevant questions. Keep responses concise and factual."
    
RAG_PROMPT = """You are an expert chatbot designed to answer questions about a scientific paper on human breast cancer and the HER-2 oncogene. Use the provided context to respond to the user's query accurately and succinctly. Keep responses concise and factual.
"""


def create_user_prompt(user_input, chatbot_prompt):
    """Prepare the user query for the chatbot by formatting it into a structured prompt."""
    messages = [
        {'role': 'system', 'content': chatbot_prompt},
        {'role': 'user', 'content': user_input}
    ]
    return messages


class ChatInterface(ABC):
    """Abstract base class for chatbot implementations.
    All chatbot classes must implement the `respond` method to handle user queries.
    """
    
    @abstractmethod
    def respond(self, user_input):
        """Generate a response to the user's query.

        Args:
            user_input (string): The user's question, expected to be related to HER2.

        Returns:
            string: The response generated by the language model.
        """
        pass


class SimpleChatBot(ChatInterface):
    """
    A basic chatbot implementation that uses a pre-trained language model
    without additional context or retrieval mechanisms.
    """

    def __init__(self, model_name='llama3.2:1b', chatbot_prompt=DEFAULT_PROMPT):
        self.model_name = model_name
        self.chatbot_prompt = chatbot_prompt

    def _create_user_prompt(self, user_input):
        """Format the user's query into a prompt suitable for the language model.

        Args:
            user_input (string): The user's question.

        Returns:
            list: A list of dictionaries containing roles and content for the prompt.
        """
        return create_user_prompt(user_input, self.chatbot_prompt)

    def respond(self, user_input):
        """Generate a response to the user's query using the language model.

        Args:
            user_input (string): The user's question, expected to be related to HER2.

        Returns:
            string: The response generated by the language model.
        """
        messages = self._create_user_prompt(user_input)
        try:
            response = ollama.chat(model=self.model_name, messages=messages)
            return response['message']['content']
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return "Sorry, I encountered an error while processing your request."


class ContextualChatBot(ChatInterface):
    """
    A chatbot implementation that uses Retrieval-Augmented Generation (RAG)
    to provide context-aware responses based on the HER2 scientific paper.
    """
    
    def __init__(self, model_name='llama3.2:1b', chatbot_prompt=RAG_PROMPT,
                 max_contexts=3, use_clinical_embeddings=False):
        """Initialize the chatbot with a language model and optional context retrieval.

        Args:
            model_name (str, optional): The language model to use. Defaults to 'llama3.2:1b'.
            chatbot_prompt (str, optional): The system prompt for the chatbot. Defaults to RAG_PROMPT.
            max_contexts (int, optional): The number of context chunks to retrieve. Defaults to 3.
            use_clinical_embeddings (bool, optional): Whether to use clinical embeddings for context retrieval. Defaults to False.
        """
        self.model_name = model_name
        self.chatbot_prompt = chatbot_prompt
        self.max_contexts = max_contexts
        
        # Load the embeddings model and vector database
        vector_db_path = os.path.join(root_dir, 'vecdb')
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError('The vector database is missing. Please run make_vector_db.py to generate it.')
        
        if use_clinical_embeddings:
            collection_name = "clinical_vector_db"
            embeddings_file = os.path.join(vector_db_path, 'clinical_embeddings_model.pkl')
        else:
            collection_name = "RAG_vector_db"
            embeddings_file = os.path.join(vector_db_path, 'embeddings.pkl')

        try:
            with open(embeddings_file, 'rb') as f:
                with warnings.catch_warnings():
                    warnings.simplefilter(action='ignore', category=FutureWarning)
                    self.embedding_model = pickle.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"The embeddings model is missing at {embeddings_file}. Please run make_vector_db.py.")

        self.vector_db = Chroma(
            persist_directory=vector_db_path, 
            embedding_function=self.embedding_model, 
            collection_name=collection_name,
        )
        # Test the vector database
        self.vector_db.similarity_search("Test", k=self.max_contexts)

    def _create_user_prompt(self, user_input, context_chunks):
        """Format the user's query and retrieved context into a structured prompt.

        Args:
            user_input (string): The user's question.
            context_chunks (list): Retrieved context chunks from the vector database.

        Returns:
            list: A list of dictionaries containing roles and content for the prompt.
        """
        context_text = "\n\n".join([doc.page_content for doc in context_chunks])
        formatted_query = f"""Use the following context from the HER2 scientific paper to answer the question.
        Context: {context_text}
        
        Question: {user_input}

        Answer:
        """
        return create_user_prompt(formatted_query, self.chatbot_prompt)

    def respond(self, user_input):
        """Generate a response to the user's query using RAG.

        Args:
            user_input (string): The user's question, expected to be related to HER2.

        Returns:
            string: The response generated by the language model.
        """
        try:
            context_chunks = self.vector_db.similarity_search(user_input, k=self.max_contexts)
            messages = self._create_user_prompt(user_input, context_chunks)
            response = ollama.chat(model=self.model_name, messages=messages)
            return response['message']['content']
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return "Sorry, I encountered an error while processing your request."


def main():
    """Main function to initialize and run the chatbot."""
    supported_models = ['llama3.2', 'llama3.2:1b']
    parser = argparse.ArgumentParser(description="HER2 Chatbot")
    parser.add_argument("--no_rag", action="store_true", 
                        help="Disable RAG and use only the basic language model.")
    parser.add_argument("--model", type=str, default="llama3.2", 
                        help="Specify the language model to use. Supported options are 'llama3.2' and 'llama3.2:1b'.")
    parser.add_argument("--clinical", action="store_true",
                        help="Enable clinical embeddings for context retrieval. Ignored if --no_rag is used.")
    args = parser.parse_args()
    
    if args.model not in supported_models:
        raise ValueError("Supported models are 'llama3.2' and 'llama3.2:1b'.")

    if args.no_rag:
        chatbot = SimpleChatBot(model_name=args.model)
    else:
        chatbot = ContextualChatBot(model_name=args.model, use_clinical_embeddings=args.clinical)
    
    print('\nWelcome to the HER2 Chatbot! Ask questions about the HER2 paper.')
    print('Type "exit" to quit.')
    while True:
        user_input = input("\nEnter your question: ")
        if user_input.lower() in ['exit', 'quit']:
            break
        print("\nGenerating response...")
        response = chatbot.respond(user_input)
        print(response)


if __name__ == "__main__":
    main()